{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1355e2d",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd0e2934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import calendar\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da5a9955",
   "metadata": {},
   "source": [
    "### Pre-Processing: EDA Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60fdcb80",
   "metadata": {},
   "source": [
    "Wrangle each of the individual files separately (One file per month & year), and return the transformed pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "139d3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_file(file_path, year_of_file):\n",
    "    ## Read the files\n",
    "    df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "\n",
    "    ## Data Cleansing\n",
    "    columns_drop = [\n",
    "        'VendorID', 'airport_fee', 'RatecodeID', 'store_and_fwd_flag', 'extra'\n",
    "        , 'mta_tax', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge'\n",
    "    ]\n",
    "    df = df.drop(columns=columns_drop)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Keep only data reported for a span of a year before the year of the file\n",
    "    df = df[df.tpep_pickup_datetime.dt.year.between(year_of_file-1, year_of_file, inclusive='both')]\n",
    "\n",
    "    # Dropoff should be after pickup\n",
    "    df = df[df['total_amount']>0]\n",
    "    df = df[(df['passenger_count']>0) & (df['passenger_count']<7)]\n",
    "    df = df[(df['trip_distance']>0) & (df['trip_distance']<=50)]\n",
    "    df = df[(df['fare_amount']>0)]\n",
    "    df = df[(df['tip_amount']<=100)]\n",
    "    \n",
    "\n",
    "    ## Transformations\n",
    "    # Get the trip duration and filter to trips with duration greater to zero\n",
    "    df['trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.seconds\n",
    "    df = df[(df['trip_duration']>0)]\n",
    "\n",
    "    # Round observations to the level of hour\n",
    "    df['pickup_datetime'] = df['tpep_pickup_datetime'].dt.floor('h')\n",
    "\n",
    "    # Add Pickup & Dropoff Zone Data\n",
    "    df['pickup_from'] = np.select( [\n",
    "            df['PULocationID'].isin([1, 132, 138]) # Location is one of the three NYC airports\n",
    "            , ~df['PULocationID'].isin([1, 132, 138]) # Location is NOT one of the three NYC airports\n",
    "        ]\n",
    "        , [\n",
    "            'Airport'\n",
    "            , 'Other'\n",
    "        ],\n",
    "        default='Unknown'\n",
    "    )\n",
    "\n",
    "    df['dropoff_at'] = np.select( [\n",
    "            df['DOLocationID'].isin([1, 132, 138]) # Location is one of the three NYC airports\n",
    "            , ~df['DOLocationID'].isin([1, 132, 138]) # Location is NOT one of the three NYC airports\n",
    "        ]\n",
    "        , [\n",
    "            'Airport'\n",
    "            , 'Other'\n",
    "        ],\n",
    "        default='Unknown'\n",
    "    )\n",
    "    \n",
    "    # Dropped this as I didn't really need the join for the analysis I was doing, just the ID's of the airports\n",
    "    # df_zones.columns = ['pickup_id', 'pickup_borough', 'pickup_zone', 'pickup_service_zone']\n",
    "    # df = df.merge(df_zones, how='left', left_on='PULocationID', right_on='pickup_id')\n",
    "    # df_zones.columns = ['dropoff_id', 'dropoff_borough', 'dropoff_zone', 'dropoff_service_zone']\n",
    "    # df = df.merge(df_zones, how='left', left_on='DOLocationID', right_on='dropoff_id')\n",
    "\n",
    "    df['trips'] = 1\n",
    "\n",
    "    # Aggregate the data at the hour level\n",
    "    df_hourly = df.groupby([\n",
    "        'pickup_datetime', 'payment_type', 'pickup_from', 'dropoff_at'\n",
    "    ]).agg({\n",
    "        'passenger_count': ['mean']\n",
    "        , 'trip_distance': ['mean']\n",
    "        , 'fare_amount': ['mean']\n",
    "        , 'tip_amount': ['mean']\n",
    "        , 'trip_duration': ['mean']\n",
    "        , 'total_amount': ['mean']\n",
    "        , 'trips': ['sum']\n",
    "    })\n",
    "\n",
    "    df_hourly = df_hourly.reset_index()\n",
    "\n",
    "    df_hourly.columns = [\n",
    "        'pickup_datetime'\n",
    "        , 'payment_type'\n",
    "        , 'pickup_from'\n",
    "        , 'dropoff_at'\n",
    "        , 'passenger_count'\n",
    "        , 'trip_distance'\n",
    "        , 'fare_amount'\n",
    "        , 'tip_amount'\n",
    "        , 'trip_duration'\n",
    "        , 'total_amount'\n",
    "        , 'trips'\n",
    "    ]\n",
    "\n",
    "    # Get the floor of the average of passengers\n",
    "    df_hourly['passenger_count'] = df_hourly['passenger_count'].apply(np.floor).astype('int')\n",
    "\n",
    "    # New Categorical Variables\n",
    "    df_hourly['congestion_category'] = np.select( [\n",
    "            df_hourly['pickup_datetime'].dt.hour.between(0, 5, inclusive='left')\n",
    "            , df_hourly['pickup_datetime'].dt.hour.between(5, 8, inclusive='left')\n",
    "            , df_hourly['pickup_datetime'].dt.hour.between(8, 13, inclusive='left')\n",
    "            , df_hourly['pickup_datetime'].dt.hour.between(13, 17, inclusive='left')\n",
    "            , df_hourly['pickup_datetime'].dt.hour.between(17, 24, inclusive='left' ) \n",
    "        ]\n",
    "        , [\n",
    "            'After Midnight Congestion'\n",
    "            , 'Early Morning Congestion'\n",
    "            , 'Leading to Noon Congestion'\n",
    "            , 'Afternoon Congestion'\n",
    "            , 'Evening Congestion'\n",
    "        ],\n",
    "        default='Unknown'\n",
    "    )\n",
    "\n",
    "    df_hourly['day_category'] = np.select( [\n",
    "            df_hourly['pickup_datetime'].dt.hour.between(0, 6, inclusive='left')\n",
    "            , df_hourly['pickup_datetime'].dt.hour.between(6, 12, inclusive='left')\n",
    "            , df_hourly['pickup_datetime'].dt.hour.between(12, 18, inclusive='left')\n",
    "            , df_hourly['pickup_datetime'].dt.hour.between(18, 24, inclusive='left')\n",
    "        ]\n",
    "        , [\n",
    "            'Early Morning'\n",
    "            , 'Late Morning'\n",
    "            , 'Afternoon'\n",
    "            , 'Evening'\n",
    "        ],\n",
    "        default='Unknown'\n",
    "    )\n",
    "\n",
    "    df_hourly['payment_type'] = np.select( [\n",
    "            df_hourly['payment_type'] == 1\n",
    "            , df_hourly['payment_type'] == 2\n",
    "            , df_hourly['payment_type'] == 3\n",
    "            , df_hourly['payment_type'] == 4\n",
    "            , df_hourly['payment_type'] == 5\n",
    "            , df_hourly['payment_type'] == 6\n",
    "        ]\n",
    "        , [\n",
    "            'Credit Card'\n",
    "            , 'Cash'\n",
    "            , 'No Charge'\n",
    "            , 'Dispute'\n",
    "            , 'Unknown'\n",
    "            , 'Voided Trip'\n",
    "        ],\n",
    "        default='Unknown'\n",
    "    )\n",
    "\n",
    "    df_hourly['source_file'] = file_path\n",
    "\n",
    "    return df_hourly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49b5881e",
   "metadata": {},
   "source": [
    "For each file apply the function specified above, and output a unioned dataframe of the result of all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eadf837",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the files\n",
    "df_zones = pd.read_csv( r\"F:\\BFD Project Data\\Complementary\\taxi+_zone_lookup.csv\")\n",
    "\n",
    "file_path=r'F:\\BFD Project Data\\Raw'\n",
    "df_hourly_combined = pd.DataFrame()\n",
    "\n",
    "for year in os.listdir(file_path):\n",
    "    for file in os.listdir('{0}/{1}'.format(file_path, year)):\n",
    "        path = '{0}\\{1}\\{2}'.format(file_path, year, file)\n",
    "        df_wrangled = wrangle_file(path, year_of_file=int(year))\n",
    "        df_hourly_combined = pd.concat([df_hourly_combined, df_wrangled])\n",
    "\n",
    "# df_keep = df_hourly_combined.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "858b7bc6",
   "metadata": {},
   "source": [
    "Transform the unioned dataset, this is the EDA Dataset. Then, export it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b0768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final Processing\n",
    "# Get only trips with a maximum of $150 as the total_amout paid\n",
    "df_hourly_combined = df_hourly_combined[df_hourly_combined['total_amount'].between(0, 150, inclusive='right')]\n",
    "\n",
    "# Get only trips with a maximum duration of an hour and 30 mins\n",
    "df_hourly_combined = df_hourly_combined[df_hourly_combined['trip_duration'].between(0, 5400, inclusive='right')]\n",
    "\n",
    "# After flooring the passenger count, remove trips with no passengers\n",
    "df_hourly_combined = df_hourly_combined[df_hourly_combined['passenger_count'] > 0]\n",
    "\n",
    "# Sort by pickup_datetime\n",
    "df_hourly_combined = df_hourly_combined.sort_values('pickup_datetime')\n",
    "\n",
    "## Write to File\n",
    "df_hourly_combined['id'] = [*range(0, df_hourly_combined.shape[0], 1)]\n",
    "df_hourly_combined.set_index(df_hourly_combined['id'])\n",
    "path_file_output = 'F:\\BFD Project Data\\Processed\\yellow_taxi_data_hourly_v6.csv'\n",
    "df_hourly_combined.to_csv(path_file_output, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ea32107",
   "metadata": {},
   "source": [
    "### Pre-Processing: Modelling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071fb550",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Post EDA Processing\n",
    "path_file_output = 'F:\\BFD Project Data\\Processed\\yellow_taxi_data_hourly_v6.csv'\n",
    "df_hourly = pd.read_csv(path_file_output, parse_dates=['pickup_datetime'], infer_datetime_format=True)\n",
    "df_hourly['pickup_hour_year'] = df_hourly['pickup_datetime'].dt.strftime('%Y, %H hrs')\n",
    "df_hourly['pickup_month_year'] = df_hourly['pickup_datetime'].dt.strftime('%B, %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "222fb991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Write to File (Ready for Modelling Datasets)\n",
    "df_hourly_amount_per_year = df_hourly[['pickup_hour_year', 'total_amount']].groupby('pickup_hour_year').mean('total_amount')\n",
    "df_hourly_amount_per_year = df_hourly_amount_per_year.reset_index()\n",
    "df_hourly_amount_per_year.to_csv('F:\\BFD Project Data\\Processed\\hourly_amount_per_year.csv', index=False)\n",
    "\n",
    "df_hourly_trips_per_year = df_hourly[['pickup_hour_year', 'trips']].groupby('pickup_hour_year').mean('trips')\n",
    "df_hourly_trips_per_year = df_hourly_trips_per_year.reset_index()\n",
    "df_hourly_trips_per_year.to_csv('F:\\BFD Project Data\\Processed\\hourly_trips_per_year.csv', index=False)\n",
    "\n",
    "# df_hourly_amount_per_year_pickup = df_hourly[['pickup_hour_year', 'pickup_from', 'total_amount']]\n",
    "# df_hourly_amount_per_year_pickup.to_csv('F:\\BFD Project Data\\Processed\\hourly_amount_per_year_pickup.csv', index=False)\n",
    "\n",
    "# df_monthly_trips_per_year = df_hourly[['pickup_month_year', 'trips']]\n",
    "# df_monthly_trips_per_year.to_csv('F:\\BFD Project Data\\Processed\\monthly_trips_per_year.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9acdc1b",
   "metadata": {},
   "source": [
    "Built this function to remove boxplot outliers based on a partition of other column (Not in use right now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f3851a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_outliers(df, column, partition_col):\n",
    "#     df_result = pd.DataFrame()\n",
    "#     partitions = df[partition_col].drop_duplicates().to_list()\n",
    "#     for partition in partitions:\n",
    "#         df_filtered = df[df[partition_col]==partition]\n",
    "#         q1 = df_filtered[column].quantile(q=0.25)\n",
    "#         q3 = df_filtered[column].quantile(q=0.75)\n",
    "#         iqr = q3 - q1   \n",
    "#         min = q1 - 1.5 * iqr\n",
    "#         max = q3 + 1.5 * iqr\n",
    "#         #df = df.drop(axis=0)\n",
    "#         df_filtered = df_filtered[(df[partition_col]==partition) & (df[column].between(min, max))]\n",
    "#         pd.concat(df_result, df_filtered)\n",
    "        \n",
    "#         print(partition)\n",
    "#         print('Q1: {0}, Q3: {1}, IQR: {2}, Min: {3}, Max: {4}'.format(q1, q3, iqr, min, max))\n",
    "#         print(df_filtered[column].describe())\n",
    "#         print()\n",
    "    \n",
    "#     return df_filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc0b6593",
   "metadata": {},
   "source": [
    "### Pandas Profiler: EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "937ce64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71784bdd10244af0bdda792a9195d8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andresespinalh\\miniconda3\\envs\\py-data\\lib\\site-packages\\multimethod\\__init__.py:315: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859c5042067b4fbda594a16181c6b489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0cf276f16c44666b44f61b5581009bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render widgets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8565aeb7784790b5b689938911c7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(Tab(children=(GridBox(children=(VBox(children=(GridspecLayout(children=(HTML(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profile = ProfileReport(df_hourly, title = 'Pandas Profiling Report')\n",
    "profile.to_widgets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "768315c921f5332c10b1a26362d9177bfe4758cae2ebdf3d51cfb3bcce92807a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
